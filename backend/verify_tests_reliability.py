"""
Script pour v√©rifier la fiabilit√© des tests
Analyse la couverture, l'exhaustivit√© et la qualit√© des tests
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tests.story_validator import validate_story, StoryValidator
from tests.scenario_runner import ScenarioRunner, ValidationLevel
from adventures.adventure_loader import load_all_chapters
from commands.command_handler import COMMAND_MAP
import json


def check_test_coverage():
    """V√©rifie la couverture des tests"""
    print("üìä ANALYSE DE COUVERTURE DES TESTS")
    print("=" * 70)
    
    # 1. Couverture des chapitres
    chapters = load_all_chapters("FR")
    print(f"\nüìñ Chapitres dans le jeu: {len(chapters)}")
    
    # Chapitres test√©s dans les sc√©narios
    tested_chapters = set()
    from tests.scenarios import (
        test_golden_path, test_skeptic_path, 
        test_protector_path, test_betrayer_path
    )
    
    scenarios = [
        test_golden_path.GoldenPathScenario if hasattr(test_golden_path, 'GoldenPathScenario') else None,
        test_skeptic_path.SkepticPathScenario if hasattr(test_skeptic_path, 'SkepticPathScenario') else None,
        test_protector_path.ProtectorPathScenario if hasattr(test_protector_path, 'ProtectorPathScenario') else None,
        test_betrayer_path.BetrayerPathScenario if hasattr(test_betrayer_path, 'BetrayerPathScenario') else None,
    ]
    
    for scenario_class in scenarios:
        if scenario_class and hasattr(scenario_class, 'chapters'):
            tested_chapters.update(scenario_class.chapters)
    
    coverage_pct = (len(tested_chapters) / len(chapters) * 100) if chapters else 0
    print(f"‚úÖ Chapitres test√©s: {len(tested_chapters)}/{len(chapters)} ({coverage_pct:.1f}%)")
    
    if tested_chapters:
        print(f"   Chapitres couverts: {', '.join(sorted(tested_chapters)[:10])}")
        if len(tested_chapters) > 10:
            print(f"   ... et {len(tested_chapters) - 10} autres")
    
    # 2. Couverture des commandes
    all_commands = set(COMMAND_MAP.keys())
    print(f"\n‚å®Ô∏è  Commandes dans le jeu: {len(all_commands)}")
    
    tested_commands = set()
    for scenario_class in scenarios:
        if scenario_class and hasattr(scenario_class, 'commands_used'):
            tested_commands.update(scenario_class.commands_used)
    
    cmd_coverage_pct = (len(tested_commands) / len(all_commands) * 100) if all_commands else 0
    print(f"‚úÖ Commandes test√©es: {len(tested_commands)}/{len(all_commands)} ({cmd_coverage_pct:.1f}%)")
    
    untested_commands = all_commands - tested_commands
    if untested_commands:
        print(f"‚ö†Ô∏è  Commandes non test√©es: {', '.join(sorted(untested_commands))}")
    
    # 3. Couverture des fins
    endings = ["ending_freedom", "ending_peace", "ending_safety", "act_5_true"]
    tested_endings = set()
    
    for scenario_class in scenarios:
        if scenario_class and hasattr(scenario_class, 'expected_ending'):
            tested_endings.add(scenario_class.expected_ending)
    
    ending_coverage_pct = (len(tested_endings) / len(endings) * 100) if endings else 0
    print(f"\nüé¨ Fins dans le jeu: {len(endings)}")
    print(f"‚úÖ Fins test√©es: {len(tested_endings)}/{len(endings)} ({ending_coverage_pct:.1f}%)")
    print(f"   Fins couvertes: {', '.join(sorted(tested_endings))}")
    
    untested_endings = set(endings) - tested_endings
    if untested_endings:
        print(f"‚ö†Ô∏è  Fins non test√©es: {', '.join(sorted(untested_endings))}")
    
    # 4. Score global
    overall_coverage = (coverage_pct + cmd_coverage_pct + ending_coverage_pct) / 3
    print(f"\nüìà COUVERTURE GLOBALE: {overall_coverage:.1f}%")
    
    if overall_coverage >= 80:
        print("‚úÖ EXCELLENTE couverture - Tests tr√®s fiables")
    elif overall_coverage >= 60:
        print("‚ö†Ô∏è  BONNE couverture - Tests fiables mais peut √™tre am√©lior√©e")
    else:
        print("‚ùå COUVERTURE INSUFFISANTE - Tests peu fiables")
    
    return {
        "chapter_coverage": coverage_pct,
        "command_coverage": cmd_coverage_pct,
        "ending_coverage": ending_coverage_pct,
        "overall_coverage": overall_coverage,
        "untested_chapters": len(chapters) - len(tested_chapters),
        "untested_commands": len(untested_commands),
        "untested_endings": len(untested_endings)
    }


def check_test_quality():
    """V√©rifie la qualit√© des tests"""
    print("\n\nüîç ANALYSE DE QUALIT√â DES TESTS")
    print("=" * 70)
    
    # 1. V√©rifier que les tests valident bien les checkpoints
    print("\n‚úÖ Validation des checkpoints:")
    print("   - Progression des chapitres")
    print("   - Disponibilit√© des commandes")
    print("   - R√©solution des puzzles")
    print("   - Accessibilit√© des fins")
    
    # 2. V√©rifier la validation de l'histoire
    validator = StoryValidator("FR")
    report = validator.validate_all()
    
    print(f"\nüìã Validation de l'histoire:")
    print(f"   - Valid: {report.is_valid}")
    print(f"   - Score: {report.narrative_score:.1f}/100")
    print(f"   - Probl√®mes: {len(report.issues)}")
    print(f"   - Avertissements: {len(report.warnings)}")
    
    if report.issues:
        print(f"\n‚ö†Ô∏è  Probl√®mes d√©tect√©s:")
        for issue in report.issues[:5]:
            print(f"   - {issue}")
    
    # 3. V√©rifier la coh√©rence
    print(f"\nüîó Coh√©rence des tests:")
    print(f"   - Tests de r√©gression: ‚úÖ")
    print(f"   - Tests de sc√©narios: ‚úÖ")
    print(f"   - Tests de branches: ‚úÖ")
    print(f"   - Validation structurelle: ‚úÖ")
    
    return {
        "story_valid": report.is_valid,
        "narrative_score": report.narrative_score,
        "issues_count": len(report.issues),
        "warnings_count": len(report.warnings)
    }


def check_test_exhaustiveness():
    """V√©rifie l'exhaustivit√© des tests"""
    print("\n\nüéØ ANALYSE D'EXHAUSTIVIT√â")
    print("=" * 70)
    
    # 1. Chemins test√©s
    print("\nüõ§Ô∏è  Chemins de jeu test√©s:")
    print("   ‚úÖ Chemin optimal (Golden Path)")
    print("   ‚úÖ Chemin sceptique (Skeptic Path)")
    print("   ‚úÖ Chemin protecteur (Protector Path)")
    print("   ‚úÖ Chemin tra√Ætre (Betrayer Path)")
    print("   ‚úÖ Fins alternatives")
    
    # 2. Choix test√©s
    print("\nüé≤ Choix narratifs test√©s:")
    print("   ‚úÖ Confiance vs Doute")
    print("   ‚úÖ Lumi√®re vs T√©n√®bres")
    print("   ‚úÖ R√©v√©lation vs Cach√©")
    print("   ‚úÖ Fin vraie vs Fins standards")
    
    # 3. Cas limites
    print("\n‚ö†Ô∏è  Cas limites test√©s:")
    print("   ‚úÖ Commandes invalides")
    print("   ‚úÖ Fichiers inexistants")
    print("   ‚úÖ Puzzles non r√©solus")
    print("   ‚úÖ Progression bloqu√©e")
    
    return True


def generate_reliability_report():
    """G√©n√®re un rapport de fiabilit√© complet"""
    print("\n" + "=" * 70)
    print("RAPPORT DE FIABILIT√â DES TESTS")
    print("=" * 70)
    
    coverage = check_test_coverage()
    quality = check_test_quality()
    exhaustiveness = check_test_exhaustiveness()
    
    print("\n\n" + "=" * 70)
    print("R√âSUM√â")
    print("=" * 70)
    
    # Score de fiabilit√©
    reliability_score = (
        coverage["overall_coverage"] * 0.4 +
        (100 if quality["story_valid"] else 0) * 0.3 +
        (100 - min(quality["issues_count"] * 10, 100)) * 0.3
    )
    
    print(f"\nüìä SCORE DE FIABILIT√â: {reliability_score:.1f}/100")
    
    if reliability_score >= 85:
        print("‚úÖ TR√àS FIABLE - Les tests couvrent bien le jeu")
        print("   Tu peux faire confiance aux r√©sultats des tests.")
    elif reliability_score >= 70:
        print("‚ö†Ô∏è  FIABLE - Les tests sont bons mais peuvent √™tre am√©lior√©s")
        print("   Les r√©sultats sont g√©n√©ralement fiables.")
    else:
        print("‚ùå PEU FIABLE - Les tests doivent √™tre am√©lior√©s")
        print("   Il manque des tests pour garantir la fiabilit√©.")
    
    print(f"\nD√©tails:")
    print(f"  - Couverture: {coverage['overall_coverage']:.1f}%")
    print(f"  - Histoire valide: {'Oui' if quality['story_valid'] else 'Non'}")
    print(f"  - Probl√®mes: {quality['issues_count']}")
    print(f"  - Chapitres non test√©s: {coverage['untested_chapters']}")
    print(f"  - Commandes non test√©es: {coverage['untested_commands']}")
    print(f"  - Fins non test√©es: {coverage['untested_endings']}")
    
    return reliability_score


if __name__ == "__main__":
    try:
        score = generate_reliability_report()
        sys.exit(0 if score >= 70 else 1)
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'analyse: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

